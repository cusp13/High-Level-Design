# ðŸ§  SYSTEM DESIGN PATTERN: SCALING READS

Topic: â€œReads are the silent killers of performanceâ€
Core Idea: In real-world systems, read operations outnumber writes dramatically â€” scaling reads properly determines system scalability.

---

ðŸš€ ðŸ’¡ WHY READS ARE HARDER TO SCALE THAN WRITES

ðŸš€ ðŸ§© The Imbalance:

| Type      | Operation Frequency   | Example                            |
| --------- | --------------------- | ---------------------------------- |
| Write | Creates/Updates data  | Posting a tweet, uploading a video |
| Read  | Fetches/Displays data | Viewing timelines, watching videos |

-> A post is created once, but read thousands or millions of times.
-> Typical ratio:

  -> Early stage: 10:1 (reads:writes)
  -> Mature systems: 100:1 or higher

ðŸš€ ðŸ§¨ Why reads hurt more:

-> Each read can touch multiple tables, joins, aggregations, filters, and indexes.
-> Caching can help, but only if data access is predictable.
-> The DB gets hammered by repetitive, redundant queries.

---

ðŸš€ ðŸ§­ THE CURE: LAYERED APPROACH TO SCALING READS

Instead of jumping straight to distributed systems, scale in layers â€” from optimizing a single DB to a distributed caching + replication architecture.

---

# ðŸ—ï¸ LAYER 1: OPTIMIZE WITHIN YOUR DATABASE

ðŸš€# 1. ðŸ§± Indexing

> â€œThe fastest query is the one that doesnâ€™t scan the whole table.â€

Indexes = shortcuts that help locate rows faster.

ðŸš€ðŸš€ ðŸ” Types of Indexes:

| Type                    | Use Case                            | Example                |
| ----------------------- | ----------------------------------- | ---------------------- |
| Single-column Index | Frequent search/filter on one field | `user_id`              |
| Composite Index     | Multiple filters used together      | `(user_id, post_id)`   |
| Partial Index       | Subset of data                      | Active rows only       |
| Covering Index      | Query fields are fully in index     | No table lookup needed |

ðŸš€ðŸš€ âš™ï¸ Best Practices:

-> Index columns in `WHERE`, `JOIN`, and `ORDER BY`.
-> Avoid indexing low-cardinality fields (e.g., gender).
-> Regularly monitor unused or duplicate indexes.

ðŸš€ðŸš€ âš ï¸ Pitfalls:

-> Over-indexing = slower writes and high storage usage.
-> Outdated statistics = poor query planning.

ðŸš€ðŸš€ ðŸ§© Real-world Examples:

-> Pinterest: Optimized feed latency by 60% via better indexing.
-> Airbnb: Uses composite indexes `(user_id, listing_id)` for quick reservation lookups.
-> GitHub: Partial indexes to skip inactive rows.

---

ðŸš€# 2. âš¡ Hardware Upgrades

Sometimes, you just need faster metal before complicating your architecture.

ðŸš€ðŸš€ ðŸš€ Options:

-> Move from HDD â†’ SSD â†’ NVMe (faster I/O).
-> Add RAM â†’ larger in-memory buffers.
-> Use faster CPU â†’ better concurrency handling.

ðŸš€ðŸš€ ðŸ§© Real-world Examples:

-> LinkedIn: NVMe SSDs improved query throughput by 40%.
-> Etsy: Increased memory â†’ faster in-memory caching.
-> Reddit: Split live DB vs analytics DB for optimized workloads.

ðŸš€ðŸš€ âš ï¸ Caution:

-> Hardware â‰  infinite scaling. Itâ€™s vertical scaling (scale-up).
-> Always pair with query optimization.

---

ðŸš€# 3. ðŸ”„ Denormalization

> â€œNormalize for writes, denormalize for reads.â€

ðŸš€ðŸš€ ðŸ§  Concept:

-> Instead of performing multiple joins, store redundant data in one place for faster reads.

ðŸš€ðŸš€ âœ… When to Use:

-> Data changes rarely but is read frequently.
-> Joins become bottlenecks (especially across distributed nodes).

ðŸš€ðŸš€ ðŸ§© Real-world Examples:

-> Facebook: Stores `likes_count` and `comment_count` directly in post record.
-> Uber: Denormalizes trip summaries for dashboards.
-> Twitter: Stores tweet + user metadata together for fast feed rendering.

ðŸš€ðŸš€ ðŸ§° How to Manage Consistency:

-> Use materialized views (auto-updated precomputed views).
-> Run background sync jobs (cron or event-driven).
-> Keep clear documentation of duplicate fields.

ðŸš€ðŸš€ âš ï¸ Pitfalls:

-> Update anomalies â€” multiple copies to maintain.
-> Increased storage and complexity.

---

# ðŸ—ï¸ LAYER 2: SCALE DATABASE HORIZONTALLY

When one server canâ€™t handle the load, distribute it across multiple machines.

---

ðŸš€# 1. ðŸ§© Read Replicas

> Separate reads from writes by replicating your database.

ðŸš€ðŸš€ âš™ï¸ Architecture:

-> Primary DB: Handles writes (source of truth).
-> Read replicas: Handle SELECT queries (eventually consistent).

ðŸš€ðŸš€ ðŸ§© Real-world Examples:

-> Instagram: Uses global read replicas to reduce latency.
-> Amazon RDS / Aurora: Offers automatic read replica management.

ðŸš€ðŸš€ âš ï¸ Issues:

| Problem                 | Description                                |
| ----------------------- | ------------------------------------------ |
| Replication Lag     | Writes not immediately visible on replicas |
| Stale Reads         | User may see outdated data                 |
| Failover Complexity | Promoting replicas is tricky               |
| Increased Costs     | More infra + sync overhead                 |

ðŸš€ðŸš€ âœ… Best Practices:

-> Use replicas for non-critical reads.
-> Monitor replication lag (e.g., `Seconds_Behind_Master` in MySQL).
-> Implement read routing (load balancer, middleware logic).

---

ðŸš€# 2. ðŸ§© Sharding

> Split one huge database into smaller, independent chunks.

ðŸš€ðŸš€ âš™ï¸ Concept:

-> Divide data horizontally (by key) across multiple DBs.
-> Each DB = â€œshardâ€ â†’ handles subset of data.

ðŸš€ðŸš€ ðŸ’¡ Example:

| Shard Key | Example                              |
| --------- | ------------------------------------ |
| User ID   | Social networks (Instagram, Twitter) |
| Video ID  | YouTube                              |
| Region    | E-commerce like Amazon               |

ðŸš€ðŸš€ âš ï¸ Challenges:

| Issue                   | Description                             |
| ----------------------- | --------------------------------------- |
| Cross-shard queries | Hard to run aggregations                |
| Hot shards          | Uneven key distribution causes overload |
| Resharding          | Moving data between shards = complex    |
| Consistency         | Distributed transactions lose atomicity |

ðŸš€ðŸš€ âœ… Best Practices:

-> Choose balanced shard keys (hash-based > range-based).
-> Use directory service / router to map users to shards.
-> Avoid cross-shard joins; use fan-out read patterns carefully.

---

# ðŸ—ï¸ LAYER 3: ADD EXTERNAL CACHING LAYERS

Caching = serving frequently requested data from faster, closer memory.

---

ðŸš€# 1. ðŸ§  Application-Level Cache (Redis, Memcached)

-> Store recent or popular query results in memory.
-> TTL-based (time to live) to expire automatically.

ðŸš€ðŸš€ ðŸ§© Real-world Examples:

-> Twitter: Caches timelines/tweets in Redis.
-> Reddit: Caches subreddit and comment threads.

ðŸš€ðŸš€ âœ… Benefits:

-> Reduces DB load drastically.
-> Low latency (microseconds vs milliseconds).

ðŸš€ðŸš€ âš ï¸ Problems:

| Issue                  | Description                            |
| ---------------------- | -------------------------------------- |
| Cache Invalidation | Hardest problem â€” when to evict/update |
| Cache Stampede     | All requests hit DB on cache expiry    |
| Hot Keys           | Popular keys overload certain nodes    |
| Memory Eviction    | Limited cache space â†’ poor hit rate    |
| Security Risks     | Bad key design may expose data         |

ðŸš€ðŸš€ ðŸ§° Mitigations:

-> Use â€œCache-asideâ€ pattern (read-through/write-through).
-> Use lock or queue to prevent stampedes.
-> Apply consistent hashing for load distribution.

---

ðŸš€# 2. ðŸŒ CDN & Edge Caching

-> Cache static or semi-static content close to the user.

ðŸš€ðŸš€ ðŸ§© Real-world Examples:

-> Netflix Open Connect: Pushes content to ISP-level nodes globally.
-> Cloudflare / Akamai / Fastly: Distribute static assets (HTML, CSS, images, videos).

ðŸš€ðŸš€ âœ… Benefits:

-> Reduces origin (server) load.
-> Improves latency by serving from nearby locations.

---

# ðŸ§© FINAL LAYERED SUMMARY

| Layer | Technique                             | Complexity | Benefit                     |
| ----- | ------------------------------------- | ---------- | --------------------------- |
| 1 | Indexing / Denormalization / Hardware | Low        | Immediate performance boost |
| 2 | Read Replicas / Sharding              | Medium     | Handle massive read loads   |
| 3 | Caching (Redis, CDN)                  | High       | Sub-ms latency at scale     |

---

# âš–ï¸ TRADE-OFFS MATRIX

| Technique            | Pros               | Cons                             |
| -------------------- | ------------------ | -------------------------------- |
| Indexing         | Simple, fast boost | Slows writes, storage cost       |
| Denormalization  | Reduces joins      | Data inconsistency risk          |
| Hardware Upgrade | Quick improvement  | Limited scalability              |
| Read Replicas    | Offloads reads     | Stale data, replication lag      |
| Sharding         | Infinite scale     | Complex ops, cross-shard queries |
| Caching          | Ultra-fast reads   | Invalidation + consistency pain  |

---

# ðŸ§° DESIGN PATTERNS INVOLVED

-> Cache-Aside Pattern â†’ load data lazily into cache
-> Read-Through / Write-Through Cache â†’ cache integrated with DB
-> CQRS (Command Query Responsibility Segregation) â†’ split reads & writes into different services
-> Materialized Views â†’ precomputed read models for speed
-> Eventual Consistency â†’ acceptable staleness in high-scale systems

---

# ðŸ§© KEY INTERVIEW INSIGHTS

-> Reads dominate in 99% of consumer systems.
-> You scale reads horizontally (replicas, caches) and writes vertically or via CQRS.
-> Always scale step-by-step, not prematurely.
-> "Cache invalidation" and "choosing shard key" are two of the hardest problems in system design.

---

# ðŸ“ QUICK REVISION NOTES

Keywords:
Indexing â€¢ Denormalization â€¢ Read Replica â€¢ Sharding â€¢ Caching â€¢ CDN â€¢ Consistency â€¢ Lag â€¢ Cache Stampede â€¢ Hot Shard â€¢ CQRS

Memory Hook:
ðŸ‘‰ ->â€œIndex â†’ Hardware â†’ Denormalize â†’ Replica â†’ Shard â†’ Cache â†’ CDNâ€->
Follow this exact sequence when designing scalable read systems.
