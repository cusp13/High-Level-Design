Elasticsearch (ES) is a distributed search and analytics engine built on top of Apache Lucene â€” a Java-based full-text search library.

Lucene = the powerful search engine core (handles indexing & searching).
Elasticsearch = the distributed system around Lucene (handles scaling, clustering, REST APIs, fault tolerance).

ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰Basic Building Blocks
Concept	Description
Index - 	A collection of related documents (like a database). Example: users, products, logs.
Document -	A JSON object stored in an index (like a row in SQL).
Field -	A key-value pair inside a document (like a column).
Mapping -	defines the data types of fields (string, number, date, etc.) like a schema.
Node - A single running Elasticsearch instance.
Cluster	- A group of nodes working together to store and search data.
Shard	- A partition of an index â€” the smallest unit of storage and search.
Replica	- A copy of a shard used for redundancy and read scaling.

ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ How Data Is Stored

Step 1: Hashing â†’ Find the Right Shard
Elasticsearch uses the document ID (1) to determine which shard will store it.
shard = hash(_id) % number_of_primary_shards
Each shard is assigned to a node in the cluster.
That node is called the primary shard for this document.
If there are replicas, Elasticsearch will also send a copy to the replica shards.

Step 2: Lucene Takes Over (Inside the Shard)
Each shard is actually a self-contained Lucene index.
Lucene is the component that builds and maintains the inverted index, which powers text search.

-> What Is an Inverted Index (Core of Lucene)
The inverted index is a data structure that maps terms â†’ documents instead of documents â†’ terms.
Example
Say we have 2 docs:
Doc 1: "apple banana"
Doc 2: "banana orange"
The inverted index looks like:
Term	Documents
apple	 -  1
banana	-  1, 2
orange	-  2

ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ How Lucene Stores Data (Segments)
Inside each shard, Lucene stores data in immutable files called segments.
When you index a document:
1. It first goes into an in-memory buffer.
2. Periodically, the buffer is flushed to disk as a new segment.
3. The segment contains: the inverted index, stored fields, term dictionary, etc

->> Background â€” What Are Segments?
A segment in Lucene is like a mini index file â€” it contains: An inverted index (terms â†’ document IDs), Stored fields (original data), Metadata
When Elasticsearch writes data, it doesnâ€™t modify old segments. Instead, it creates a new segment every time new data comes in. 
So Lucene is append-only â†’ new data = new file; updates/deletes are â€œmarkedâ€, not modified in-place. 
Lucene never modifies data in place on disk.
Whenever you add, update, or delete a document, Lucene writes a new segment (a mini-index file). This append-only design gives it:
Speed â€” writes are sequential, not random disk I/O.
Safety â€” avoids file corruption on crash.
Consistency â€” no locks on old data

âœ… Benefit 1: Faster Concurrent Reads and Writes
ğŸ”¹ Problem (in normal databases)
In traditional databases (like MySQL), updates change existing records on disk.
That means:
If one thread is writing, another reading thread might need to wait (locking).
Disk blocks are rewritten â†’ slower I/O.
ğŸ”¹ Solution (in Lucene)
Because segments are never modified once written, readers and writers never block each other:
Writer thread â†’ appends new segments.
Reader thread â†’ keeps reading old segments safely (they never change).
When new segments are ready (after a refresh), readers simply start using them too. (that' why it is eventual consistency).
âœ… Result â†’ High throughput for both reads and writes concurrently.
ğŸš€ Benefit 2: Simplified Crash Recovery
ğŸ”¹ Problem (if data were mutable)
If Lucene modified files in place and a crash happened mid-write, the files could become corrupted.
Youâ€™d lose track of what was half-written.
ğŸ”¹ Solution (append-only + translog)
Because Lucene writes new immutable segments, each is either:
Fully written (safe), or
Not yet visible (ignored).
On top of that, Elasticsearch maintains a transaction log (translog) â€” like a safety journal:
When new data is added, itâ€™s also written to the translog.
After a crash, Elasticsearch replays the translog to restore missing segments.
âœ… Result â†’ Very safe and simple recovery, no complex â€œrollbackâ€ needed.

Case 1: Document doesnâ€™t exist
â†’ A new document is written into an in-memory buffer.
â†’ Later, that buffer is flushed to disk as a new segment (a file with inverted index).
Case 2: Document already exists
â†’ Elasticsearch treats it as a delete + add operation:
The old document with _id = 123 is marked as deleted in its segment (but not physically removed yet).
A new version of the document is appended in a new segment.
So yes â€” the old one still exists physically on disk until Lucene runs merge operations.
ğŸ§¹ Segment merging â€” the cleanup
Lucene runs a background merge process that:
Combines smaller segments into bigger ones for efficiency.
Physically drops deleted docs from the new merged segment.
Thatâ€™s how the old â€œdeletedâ€ document eventually disappears.


ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰The Lifecycle of a Document
ğŸª¶ Step 1ï¸âƒ£: Indexing â€” Writing to Memory and Translog
When you send a new document to Elasticsearch like:
POST /users/_doc/1
{
  "name": "Divyansh",
  "role": "Developer"
}

Elasticsearch doesnâ€™t immediately write that to disk. Instead, it does two things in memory and for durability:
Memory Buffer (In-Memory Segment):
1. The document is parsed and inverted index entries (like tokens â†’ docIDs) are written into a memory buffer. Think of this as a temporary area holding your document before itâ€™s made searchable.
2. Translog (Transaction Log): At the same time, a copy of your operation (the JSON body) is written into a log file on disk.
This ensures if the node crashes, Elasticsearch can replay these operations from the translog and recover your data.
ğŸ”¹ Why both?
Memory buffer = fast writes (RAM) and Translog = durability (disk)

ğŸª¶ Step 2ï¸âƒ£: Refresh â€” Making Data Searchable
By default, every 1 second, Elasticsearch performs a refresh.
During refresh:
The in-memory buffer is turned into a new Lucene segment.
This segment is read-only and searchable.
The segment is then added to the list of active segments for that shard.
â†’ At this point, your document becomes visible in search results.
Thatâ€™s why Elasticsearch is near-real-time (NRT) â€” not instantly real-time, but usually searchable within 1 second.

ğŸ’¾ Step 3ï¸âƒ£: Flushing â€” Making Data Permanent
A flush happens:
Periodically (default: every 30 minutes), Or when the translog gets too large, Or manually (POST /index/_flush).
During flushing:
All in-memory segments are written permanently to disk. The translog is cleared (since its data is now safely on disk).
So, flush is what makes your data fully persisted and recoverable even if the node restarts.
ğŸ§  In short:
Refresh â†’ makes data searchable (in-memory â†’ segment)
Flush â†’ makes data durable (clears translog)

ğŸ§¹ Step 4ï¸âƒ£: Merging â€” Cleaning and Optimizing Segments
Now over time, you get many small segments because every refresh creates one.
Too many segments â†’ slower search
(Elasticsearch must query all segments and merge results.) So, Lucene performs segment merging in the background Combines small segments into bigger ones.
During merging:
Deleted documents are dropped. Old versions of updated docs are removed. Data is compressed and optimized.
After merging, only a few large segments remain â†’ faster queries and less disk usage.
ğŸ”¹ Merging is automatic, but itâ€™s I/O intensive.
So during heavy write loads, ğŸ¥¹ Elasticsearch may slow down slightly because of merges.

Problem when writes are heavy and reads are compromised -> When you have both heavy writes and reads on the same index, the background merges and refreshes can slow down searches because each search must touch multiple small segments, including newly written ones. Hereâ€™s how you can improve read performance without breaking write operations:
1ï¸âƒ£ Use Separate Indices for Writes and Reads (ğŸš€Hot-Warm ArchitectureğŸš€)
Hot index: actively receiving writes.
Warm/Read-only index: optimized for searches, receives fewer writes.
Flow:
Write to the hot index. Periodically, move old documents to a read-optimized index. Search mostly on the read-only index.
âœ… Benefits:
Read queries donâ€™t compete with heavy writes.
You can apply more aggressive optimizations (like force merge) on a read-only index.
mixed read/write:
Problem	 - Solution
Too many small segments due to frequent writes	 - Increase refresh_interval during heavy writes
Old/updated docs slow searches	- Force merge old segments (read-only)
Reads compete with writes - 	Use replicas to distribute search load
Large index slows search	- Use multiple smaller indices (time-based / hot-warm)

ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ How Searching works
1ï¸âƒ£ Coordinating Node Receives the Query
When you run: GET /users/_search?q=city:Delhi
The node you hit acts as the coordinating node, even if it also holds shards.
Its responsibilities:
1. Determine which shards (primary or replica) are relevant for this index.
2. Forward the query to each shard in parallel.
Think of it like a project manager: it doesnâ€™t do the work itself but coordinates all the workers (shards).
2ï¸âƒ£ Shards Execute the Search Locally
Each shard:
1. Has its own Lucene index (a mini inverted index).
2. Executes the query independently, without caring about other shards.
3. Returns top N matching documents for that shard (doc IDs + scores).
3ï¸âƒ£ Coordinating Node Merges Shard Results
Once all shards respond, the coordinating node:
Merges all results.
1. Sorts globally (by score, date, etc.).
2. Returns the final ranked list to the client.
3. This merging step ensures you get a single, consistent result, even though each shard worked independently.
ğŸ’¡ Tip: If your query is heavy and shards are big, this merging step can become a bottleneckâ€”another reason to optimize shards and replicas for reads.
1ï¸âƒ£ Shards: Splitting Your Index
Shards are the basic unit of data storage in Elasticsearch. Each shard is a Lucene index.
By default, each index has 5 primary shards (older default) but this depends on your Elasticsearch version and index size.
âš¡ Optimization Tips:
Right shard size
Too small â†’ too many shards â†’ overhead in cluster state and memory.
Too big â†’ slower queries because a single shard becomes a bottleneck.
Rule of thumb: 20â€“50 GB per shard is often optimal for read-heavy workloads.
Avoid over-sharding
If your index is small (<50 GB), 1â€“2 shards are enough.
Every shard adds file handles, memory overhead, and query coordination cost.
Shard awareness
For heavy reads, more shards allow Elasticsearch to parallelize queries across them.
But donâ€™t overdo it â†’ diminishing returns.

2ï¸âƒ£ Replicas: Scaling Reads
Replicas are copies of primary shards. They are readable, which allows distributing search requests.
âš¡ Optimization Tips:
Increase replicas for read-heavy systems
Each replica can serve search queries â†’ more replicas = more read throughput.
Example: 5 primary shards + 2 replicas = 15 shards serving reads in parallel.
Placement
Ensure replicas are on different nodes than their primaries â†’ failover + distributed read load.
Balancing reads & writes
Writes update all replicas, so increasing replicas increases write overhead slightly.
If you have mostly reads, this trade-off is fine.
Summary
| Component      | Recommendation                                 |
| -------------- | ---------------------------------------------- |
| Primary Shards | 20â€“50 GB per shard, not too many               |
| Replicas       | Increase replicas to scale read throughput     |
| Shard Size     | Avoid very small or very large shards          |
| Node Placement | Distribute primaries and replicas across nodes |
| Query Strategy | Use routing and caching when possible          |


ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€Optimizing any large size index:
1ï¸âƒ£ Shard Size & Count
For 150M documents, the total data size in GB matters more than the document count, but assuming an average doc size of ~1â€“2 KB, your index is roughly 150â€“300 GB.
Using the 20â€“50 GB per shard rule:
Youâ€™d need 6â€“15 primary shards.
If you currently have fewer, your shards may be too big â†’ slower queries.
If you have many more, your shards may be too small â†’ cluster overhead.
âœ… Action: Check _cat/indices?v for shard size and count. Adjust only if shards are too big/small.
2ï¸âƒ£ Replicas
For read-heavy workloads, replicas help:
Each replica serves reads.
With 1 replica, your read capacity doubles.
âœ… Action: If you have mostly reads, consider 1â€“2 replicas, depending on cluster resources.
3ï¸âƒ£ Mappings & Fields
Storing fields as both text and keyword is common. But for heavy read indexes, check:
Are all fields analyzed as text really needed for full-text search?
Could some text fields be keyword only if you never search them with full-text queries?
Reduce unnecessary text fields to save indexing overhead.
4ï¸âƒ£ Refresh Interval
Default: 1s. Frequent writes cause segment creation â†’ refreshes can impact search performance.
For a mostly read-heavy system:
Increase refresh interval to 5â€“30s.
Writes are slightly delayed in visibility, but reads get smoother.
5ï¸âƒ£ Index & Query Optimization
Avoid deep pagination (from + size very large). Use search_after for deep scrolling.
Enable doc values for keyword/aggregation fields.
Consider query caching for repeated queries.







