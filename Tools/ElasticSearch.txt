Short Notes:-
⚙️ Elasticsearch (ES) — Developer Notes
Elasticsearch (ES) is a distributed search and analytics engine built on top of Apache Lucene — a Java-based full-text search library.

Lucene = the powerful search engine core (handles indexing & searching).
Elasticsearch = the distributed system around Lucene (handles scaling, clustering, REST APIs, fault tolerance).

👉👉👉👉👉👉👉👉 Basic Building Blocks

Index -  A collection of related documents (like a database). Example: users, products, logs.
Document -  A JSON object stored in an index (like a row in SQL).
Field -  A key-value pair inside a document (like a column).
Mapping - defines the data types of fields (string, number, date, etc.) like a schema.
Node - A single running Elasticsearch instance.
Cluster - A group of nodes working together to store and search data.
Shard - A partition of an index — the smallest unit of storage and search.
Replica - A copy of a shard used for redundancy and read scaling.

👉👉👉👉👉👉👉👉 How Data Is Stored

Step 1: Hashing → Find the Right Shard
Elasticsearch uses the document ID (_id) to determine which shard will store it.
shard = hash(_id) % number_of_primary_shards

Step 2: Lucene Takes Over (Inside the Shard)
Each shard is actually a self-contained Lucene index.
Lucene is the component that builds and maintains the inverted index, which powers text search.

Inverted Index Example:
Doc 1: "apple banana"
Doc 2: "banana orange"

Term     Documents
apple    1
banana   1, 2
orange   2

👉👉👉👉👉👉👉 How Lucene Stores Data (Segments)
Lucene stores data in immutable files called segments.
When you index a document:
1. It first goes into an in-memory buffer.
2. Periodically, the buffer is flushed to disk as a new segment.
3. The segment contains: the inverted index, stored fields, term dictionary, etc.

Lucene is append-only → new data = new file; updates/deletes are “marked”, not modified in-place.

✅ Benefit 1: Faster Concurrent Reads and Writes
Because segments are never modified once written, readers and writers never block each other.

✅ Benefit 2: Simplified Crash Recovery
Lucene writes immutable segments. Elasticsearch also maintains a transaction log (translog) to replay operations after crash recovery.

Case 1: Document doesn’t exist → Written to in-memory buffer, then flushed to disk.
Case 2: Document exists → Delete + Add (old doc marked deleted, new doc written).

🧹 Segment merging — Combines smaller segments into bigger ones and drops deleted docs.

👉👉👉👉👉👉👉👉 The Lifecycle of a Document

🪶 Step 1️⃣: Indexing — Writing to Memory and Translog
Memory buffer = fast writes (RAM)
Translog = durability (disk)

🪶 Step 2️⃣: Refresh — Making Data Searchable
Every 1 second (by default), memory buffer → segment → searchable.

💾 Step 3️⃣: Flushing — Making Data Permanent
Flush writes segments permanently to disk and clears translog.

🧹 Step 4️⃣: Merging — Cleaning and Optimizing Segments
Lucene merges small segments into bigger ones for better performance.

👉👉👉👉👉👉👉👉 Problem: Heavy Writes vs Read Performance
Solutions:
1. Use Separate Indices (Hot-Warm Architecture)
2. Increase refresh_interval during heavy writes
3. Force merge read-only segments
4. Use replicas to distribute reads
5. Use multiple smaller indices

👉👉👉👉👉👉 How Searching Works

1️⃣ Coordinating Node Receives the Query
Determines which shards are relevant, forwards query to each shard.

2️⃣ Shards Execute Search Locally
Each shard executes the query independently and returns top N results.

3️⃣ Coordinating Node Merges Results
Merges, sorts, and returns the final ranked list.

👉👉👉👉👉👉 Shards and Replicas

Shards - Basic unit of storage (mini Lucene index).
Optimal shard size: 20–50 GB.
Avoid too many small shards.

Replicas - Copies of primary shards, used for scaling reads.
Ensure replicas are on different nodes.

Summary:
Primary Shards → 20–50 GB
Replicas → 1–2 for read-heavy systems

🚀🚀🚀🚀🚀🚀 Optimizing Large Indexes (e.g., 150M docs)
1️⃣ Shard Size & Count → 20–50 GB per shard (6–15 shards for 150–300 GB).
2️⃣ Replicas → 1–2 replicas for read-heavy systems.
3️⃣ Mappings → Remove unnecessary text fields.
4️⃣ Refresh Interval → Increase to 5–30s.
5️⃣ Query Optimization → Use search_after, caching, and doc values.

🚀🚀🚀🚀🚀🚀🚀 Cluster & Node Types

Master Node → Manages cluster state.
Data Node → Stores shards and executes queries.
Coordinating Node → Routes requests and merges results.
Ingest Node → Handles preprocessing and pipelines.

🚀🚀🚀🚀🚀 Scaling in Elasticsearch

Horizontal Scaling → via shards across nodes.
Replication → improves read performance and fault tolerance.
Routing → determines which shard stores a document.

🚀🚀🚀🚀🚀 Key Tradeoffs in ES Operations

Operation       How ES Handles                            Tradeoff
Indexing        Append-only → new docs go to shard         Very fast
Searching       Distributed → parallel query to shards     Scales well but adds overhead
Updating        Delete + Reinsert in new segment           Expensive, cleanup via merge
Deleting        Lazy delete, cleaned up in merge           Space freed later

Notes:
- Updates & Deletes are lazy (due to immutable segments)
- Merges handle cleanup
- Heavy updates/deletes can cause segment bloat and slow reads

----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

🚀🚀🚀🚀🚀🚀🚀🚀🚀Detailed Notes:-
Elasticsearch (ES) is a distributed search and analytics engine built on top of Apache Lucene, a Java-based full-text search library.

Lucene = the powerful search engine core (handles indexing & searching).
Elasticsearch = the distributed system around Lucene (handles scaling, clustering, REST APIs, fault tolerance).

👉👉👉👉👉👉👉👉Basic Building Blocks
Concept	Description
Index - 	A collection of related documents (like a database). Example: users, products, logs.
Document -	A JSON object stored in an index (like a row in SQL).
Field -	A key-value pair inside a document (like a column).
Mapping -	defines the data types of fields (string, number, date, etc.) like a schema.
Node - A single running Elasticsearch instance.
Cluster	- A group of nodes working together to store and search data.
Shard	- A partition of an index — the smallest unit of storage and search.
Replica	- A copy of a shard used for redundancy and read scaling.

👉👉👉👉👉👉👉👉 How Data Is Stored

Step 1: Hashing → Find the Right Shard
Elasticsearch uses the document ID (1) to determine which shard will store it.
shard = hash(_id) % number_of_primary_shards
Each shard is assigned to a node in the cluster.
That node is called the primary shard for this document.
If there are replicas, Elasticsearch will also send a copy to the replica shards.

Step 2: Lucene Takes Over (Inside the Shard)
Each shard is actually a self-contained Lucene index.
Lucene is the component that builds and maintains the inverted index, which powers text search.

-> What Is an Inverted Index (Core of Lucene)
The inverted index is a data structure that maps terms → documents instead of documents → terms.
Example
Say we have 2 docs:
Doc 1: "apple banana"
Doc 2: "banana orange"
The inverted index looks like:
Term	Documents
apple	 -  1
banana	-  1, 2
orange	-  2

👉👉👉👉👉👉👉 How Lucene Stores Data (Segments)
Inside each shard, Lucene stores data in immutable files called segments.
When you index a document:
1. It first goes into an in-memory buffer.
2. Periodically, the buffer is flushed to disk as a new segment.
3. The segment contains: the inverted index, stored fields, term dictionary, etc

->> Background — What Are Segments?
A segment in Lucene is like a mini index file — it contains: An inverted index (terms → document IDs), Stored fields (original data), Metadata
When Elasticsearch writes data, it doesn’t modify old segments. Instead, it creates a new segment every time new data comes in. 
So Lucene is append-only → new data = new file; updates/deletes are “marked”, not modified in-place. 
Lucene never modifies data in place on disk.
Whenever you add, update, or delete a document, Lucene writes a new segment (a mini-index file). This append-only design gives it:
Speed — writes are sequential, not random disk I/O.
Safety — avoids file corruption on crash.
Consistency — no locks on old data

✅ Benefit 1: Faster Concurrent Reads and Writes
🔹 Problem (in normal databases)
In traditional databases (like MySQL), updates change existing records on disk.
That means:
If one thread is writing, another reading thread might need to wait (locking).
Disk blocks are rewritten → slower I/O.
🔹 Solution (in Lucene)
Because segments are never modified once written, readers and writers never block each other:
Writer thread → appends new segments.
Reader thread → keeps reading old segments safely (they never change).
When new segments are ready (after a refresh), readers simply start using them too. (that' why it is eventual consistency).
✅ Result → High throughput for both reads and writes concurrently.
🚀 Benefit 2: Simplified Crash Recovery
🔹 Problem (if data were mutable)
If Lucene modified files in place and a crash happened mid-write, the files could become corrupted.
You’d lose track of what was half-written.
🔹 Solution (append-only + translog)
Because Lucene writes new immutable segments, each is either:
Fully written (safe), or
Not yet visible (ignored).
On top of that, Elasticsearch maintains a transaction log (translog) — like a safety journal:
When new data is added, it’s also written to the translog.
After a crash, Elasticsearch replays the translog to restore missing segments.
✅ Result → Very safe and simple recovery, no complex “rollback” needed.

Case 1: Document doesn’t exist
→ A new document is written into an in-memory buffer.
→ Later, that buffer is flushed to disk as a new segment (a file with inverted index).
Case 2: Document already exists
→ Elasticsearch treats it as a delete + add operation:
The old document with _id = 123 is marked as deleted in its segment (but not physically removed yet).
A new version of the document is appended in a new segment.
So yes — the old one still exists physically on disk until Lucene runs merge operations.
🧹 Segment merging — the cleanup
Lucene runs a background merge process that:
Combines smaller segments into bigger ones for efficiency.
Physically drops deleted docs from the new merged segment.
That’s how the old “deleted” document eventually disappears.


👉👉👉👉👉👉👉👉The Lifecycle of a Document
🪶 Step 1️⃣: Indexing — Writing to Memory and Translog
When you send a new document to Elasticsearch like:
POST /users/_doc/1
{
  "name": "Divyansh",
  "role": "Developer"
}

Elasticsearch doesn’t immediately write that to disk. Instead, it does two things in memory and for durability:
Memory Buffer (In-Memory Segment):
1. The document is parsed and inverted index entries (like tokens → docIDs) are written into a memory buffer. Think of this as a temporary area holding your document before it’s made searchable.
2. Translog (Transaction Log): At the same time, a copy of your operation (the JSON body) is written into a log file on disk.
This ensures if the node crashes, Elasticsearch can replay these operations from the translog and recover your data.
🔹 Why both?
Memory buffer = fast writes (RAM) and Translog = durability (disk)

🪶 Step 2️⃣: Refresh — Making Data Searchable
By default, every 1 second, Elasticsearch performs a refresh.
During refresh:
The in-memory buffer is turned into a new Lucene segment.
This segment is read-only and searchable.
The segment is then added to the list of active segments for that shard.
→ At this point, your document becomes visible in search results.
That’s why Elasticsearch is near-real-time (NRT) — not instantly real-time, but usually searchable within 1 second.

💾 Step 3️⃣: Flushing — Making Data Permanent
A flush happens:
Periodically (default: every 30 minutes), Or when the translog gets too large, Or manually (POST /index/_flush).
During flushing:
All in-memory segments are written permanently to disk. The translog is cleared (since its data is now safely on disk).
So, flush is what makes your data fully persisted and recoverable even if the node restarts.
🧠 In short:
Refresh → makes data searchable (in-memory → segment)
Flush → makes data durable (clears translog)

🧹 Step 4️⃣: Merging — Cleaning and Optimizing Segments
Now over time, you get many small segments because every refresh creates one.
Too many segments → slower search
(Elasticsearch must query all segments and merge results.) So, Lucene performs segment merging in the background Combines small segments into bigger ones.
During merging:
Deleted documents are dropped. Old versions of updated docs are removed. Data is compressed and optimized.
After merging, only a few large segments remain → faster queries and less disk usage.
🔹 Merging is automatic, but it’s I/O intensive.
So during heavy write loads, 🥹 Elasticsearch may slow down slightly because of merges.

Problem when writes are heavy and reads are compromised -> When you have both heavy writes and reads on the same index, the background merges and refreshes can slow down searches because each search must touch multiple small segments, including newly written ones. Here’s how you can improve read performance without breaking write operations:
1️⃣ Use Separate Indices for Writes and Reads (🚀Hot-Warm Architecture🚀)
Hot index: actively receiving writes.
Warm/Read-only index: optimized for searches, receives fewer writes.
Flow:
Write to the hot index. Periodically, move old documents to a read-optimized index. Search mostly on the read-only index.
✅ Benefits:
Read queries don’t compete with heavy writes.
You can apply more aggressive optimizations (like force merge) on a read-only index.
mixed read/write:
Problem	 - Solution
Too many small segments due to frequent writes	 - Increase refresh_interval during heavy writes
Old/updated docs slow searches	- Force merge old segments (read-only)
Reads compete with writes - 	Use replicas to distribute search load
Large index slows search	- Use multiple smaller indices (time-based / hot-warm)

👉👉👉👉👉👉 How Searching works
1️⃣ Coordinating Node Receives the Query
When you run: GET /users/_search?q=city:Delhi
The node you hit acts as the coordinating node, even if it also holds shards.
Its responsibilities:
1. Determine which shards (primary or replica) are relevant for this index.
2. Forward the query to each shard in parallel.
Think of it like a project manager: it doesn’t do the work itself but coordinates all the workers (shards).
2️⃣ Shards Execute the Search Locally
Each shard:
1. Has its own Lucene index (a mini inverted index).
2. Executes the query independently, without caring about other shards.
3. Returns top N matching documents for that shard (doc IDs + scores).
3️⃣ Coordinating Node Merges Shard Results
Once all shards respond, the coordinating node:
Merges all results.
1. Sorts globally (by score, date, etc.).
2. Returns the final ranked list to the client.
3. This merging step ensures you get a single, consistent result, even though each shard worked independently.
💡 Tip: If your query is heavy and shards are big, this merging step can become a bottleneck—another reason to optimize shards and replicas for reads.
1️⃣ Shards: Splitting Your Index
Shards are the basic unit of data storage in Elasticsearch. Each shard is a Lucene index.
By default, each index has 5 primary shards (older default) but this depends on your Elasticsearch version and index size.
⚡ Optimization Tips:
Right shard size
Too small → too many shards → overhead in cluster state and memory.
Too big → slower queries because a single shard becomes a bottleneck.
Rule of thumb: 20–50 GB per shard is often optimal for read-heavy workloads.
Avoid over-sharding
If your index is small (<50 GB), 1–2 shards are enough.
Every shard adds file handles, memory overhead, and query coordination cost.
Shard awareness
For heavy reads, more shards allow Elasticsearch to parallelize queries across them.
But don’t overdo it → diminishing returns.

2️⃣ Replicas: Scaling Reads
Replicas are copies of primary shards. They are readable, which allows distributing search requests.
⚡ Optimization Tips:
Increase replicas for read-heavy systems
Each replica can serve search queries → more replicas = more read throughput.
Example: 5 primary shards + 2 replicas = 15 shards serving reads in parallel.
Placement
Ensure replicas are on different nodes than their primaries → failover + distributed read load.
Balancing reads & writes
Writes update all replicas, so increasing replicas increases write overhead slightly.
If you have mostly reads, this trade-off is fine.
Summary
| Component      | Recommendation                                 |
| -------------- | ---------------------------------------------- |
| Primary Shards | 20–50 GB per shard, not too many               |
| Replicas       | Increase replicas to scale read throughput     |
| Shard Size     | Avoid very small or very large shards          |
| Node Placement | Distribute primaries and replicas across nodes |
| Query Strategy | Use routing and caching when possible          |


🚀🚀🚀🚀🚀🚀Optimizing any large size index:
1️⃣ Shard Size & Count
For 150M documents, the total data size in GB matters more than the document count, but assuming an average doc size of ~1–2 KB, your index is roughly 150–300 GB.
Using the 20–50 GB per shard rule:
You’d need 6–15 primary shards.
If you currently have fewer, your shards may be too big → slower queries.
If you have many more, your shards may be too small → cluster overhead.
✅ Action: Check _cat/indices?v for shard size and count. Adjust only if shards are too big/small.
2️⃣ Replicas
For read-heavy workloads, replicas help:
Each replica serves reads.
With 1 replica, your read capacity doubles.
✅ Action: If you have mostly reads, consider 1–2 replicas, depending on cluster resources.
3️⃣ Mappings & Fields
Storing fields as both text and keyword is common. But for heavy read indexes, check:
Are all fields analyzed as text really needed for full-text search?
Could some text fields be keyword only if you never search them with full-text queries?
Reduce unnecessary text fields to save indexing overhead.
4️⃣ Refresh Interval
Default: 1s. Frequent writes cause segment creation → refreshes can impact search performance.
For a mostly read-heavy system:
Increase refresh interval to 5–30s.
Writes are slightly delayed in visibility, but reads get smoother.
5️⃣ Index & Query Optimization
Avoid deep pagination (from + size very large). Use search_after for deep scrolling.
Enable doc values for keyword/aggregation fields.
Consider query caching for repeated queries.

🚀🚀🚀🚀🚀🚀🚀 Cluster & Node Types

Your table already lists the roles. Let’s expand:
Master Node
1. Manages cluster state (metadata like shard locations, mappings).
2. Makes decisions about shard allocation, adding/removing nodes.
3. Minimal CPU/memory needed unless cluster is huge.

Data Node
1. Stores primary & replica shards.
2. Executes searches, aggregations, indexing.
3. Most resource-intensive.

Coordinating Node
1. Acts as a router.
2. Receives queries, sends to relevant shards, merges results.
3. Does not store data. Good for load balancing in large clusters.

Ingest Node
1. Handles data preprocessing, pipelines, enrichments.
2. Useful for log processing, transformations before indexing.

🚀🚀🚀🚀🚀 Scaling in Elasticsearch
✅Horizontal Scaling
1. Achieved via shards.
2. More shards → more nodes can participate → parallelism.
3. Watch out: too many small shards → overhead.
✅Replication
1. Improves read performance and fault tolerance.
2. Each replica is automatically synced with its primary.
✅Routing
1. Elasticsearch decides which shard a document goes to using document ID hashing.
2. Ensures queries know exactly which shard to look in.

4️⃣ Key Tradeoffs in Operations
Operation	How Elasticsearch Handles	Tradeoff
Indexing (write)	Append-only → new docs go to shard	Very fast
Searching (read)	Distributed → queries sent to all shards in parallel	Scales well with replicas, but too many shards can add overhead
Updating	Update = delete + reinsert in new segment	Expensive, old segment marked deleted → cleaned in merge
Deleting	Lazy delete → marked as deleted	Doesn’t free space immediately; merges handle cleanup


🚀🚀🚀🚀🚀🚀 Key Tradeoffs in ES Operations
Operation	: How Elasticsearch Handles	Tradeoff
1. Indexing (write)	Append-only → new docs go to shard	Very fast
2. Searching (read)	Distributed → queries sent to all shards in parallel	Scales well with replicas, but too many shards can add overhead
3. Updating	Update = delete + reinsert in new segment	Expensive, old segment marked deleted → cleaned in merge
4. Deleting	Lazy delete → marked as deleted	Doesn’t free space immediately; merges handle cleanup


Important Notes:
1. Updates & Deletes are “lazy” because Lucene uses immutable segments.
2. Merges run in background → old deleted docs are removed.
3. Heavy updates/deletes → can cause segment bloat, slowing reads.





